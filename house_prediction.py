# -*- coding: utf-8 -*-
"""house prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13VAXm17jGLJvMuMT9HGgsjMQ8hrjrImN
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

df=pd.read_csv('/content/housing.csv')

df

df.info()

df.dropna(inplace=True)

df.info()

from sklearn.model_selection import train_test_split

x=df.drop(['median_house_value'],axis=1)
y=df['median_house_value']

x

y

x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2)

train_df=x_train.join(y_train)

train_df

train_df.hist()

train_df.hist(figsize=(15,12))

train_df.corr()

plt.figure(figsize=(15,8))
sns.heatmap(train_df.corr(),annot=True,cmap="YlGnBu")

train_df['total_rooms']=np.log(train_df['total_rooms']+1)
train_df['total_bedrooms']=np.log(train_df['total_bedrooms']+1)
train_df['population']=np.log(train_df['population']+1)
train_df['households']=np.log(train_df['households']+1)

train_df.hist(figsize=(15,8))

train_df.ocean_proximity.value_counts()

train_df=train_df.join(pd.get_dummies(train_df.ocean_proximity)).drop(['ocean_proximity'],axis=1)

train_df

plt.figure(figsize=(15,8))
sns.heatmap(train_df.corr(),annot=True,cmap="YlGnBu")

plt.figure(figsize=(15,8))
sns.scatterplot(x='latitude',y='longitude',data=train_df,hue='median_house_value',palette='coolwarm')

train_df['bedroom_ratio']=train_df['total_bedrooms']/train_df['total_rooms']
train_df['household_rooms']=train_df['total_rooms']/train_df['households']

plt.figure(figsize=(15,8))
sns.heatmap(train_df.corr(),annot=True,cmap="YlGnBu")

#linear regression model
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler

scaler=StandardScaler()

x_train,y_train=train_df.drop(['median_house_value'],axis=1),train_df['median_house_value']

x_train_s=scaler.fit_transform(x_train)
reg=LinearRegression()
reg.fit(x_train_s,y_train)

test_df=x_test.join(y_test)

test_df['total_rooms']=np.log(test_df['total_rooms']+1)
test_df['total_bedrooms']=np.log(test_df['total_bedrooms']+1)
test_df['population']=np.log(test_df['population']+1)
test_df['households']=np.log(test_df['households']+1)


test_df=test_df.join(pd.get_dummies(test_df.ocean_proximity)).drop(['ocean_proximity'],axis=1)

test_df['bedroom_ratio']=test_df['total_bedrooms']/test_df['total_rooms']
test_df['household_rooms']=test_df['total_rooms']/test_df['households']

x_test,y_test=test_df.drop(['median_house_value'],axis=1),test_df['median_house_value']

x_test_s=scaler.transform(x_test)

test_df

reg.score(x_test_s,y_test)

# Random Forest

from sklearn.ensemble import RandomForestRegressor

forest = RandomForestRegressor()

# Fit the model with training data
forest.fit(x_train, y_train)

forest.score(x_test,y_test)

from sklearn.ensemble import RandomForestRegressor

forest = RandomForestRegressor()

# Fit the model with training data
forest.fit(x_train_s, y_train)

forest.score(x_test_s,y_test)

#Cross validation (k-folds)

from sklearn.model_selection import GridSearchCV

forest= RandomForestRegressor()

param_grid={
    "n_estimators":[30,50,100],
    "max_features":[8,12,20],
    }

grid_search= GridSearchCV(forest,param_grid,cv=5,scoring='neg_mean_squared_error',
                          return_train_score=True)
grid_search.fit(x_train_s,y_train)

best_forest=grid_search.best_estimator_

best_forest.score(x_test_s,y_test)

